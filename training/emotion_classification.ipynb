{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Emotion Classification with PyTorch\n",
        "\n",
        "This notebook builds a small neural network using **PyTorch** to classify a song's **emotion** from audio-related features.\n",
        "\n",
        "We assume we start from a `pandas` DataFrame `df` with columns:\n",
        "\n",
        "- `Popularity`\n",
        "- `Energy`\n",
        "- `Danceability`\n",
        "- `Positiveness`\n",
        "- `Speechiness`\n",
        "- `Liveness`\n",
        "- `Acousticness`\n",
        "- `Instrumentalness`\n",
        "\n",
        "and a target column:\n",
        "\n",
        "- `Emotion` (categorical label)\n",
        "\n",
        "The steps are:\n",
        "\n",
        "1. Segment the data into **training** and **test** sets.\n",
        "2. Build a small **neural network** in PyTorch.\n",
        "3. Train the network using **k-fold cross-validation** on the training set.\n",
        "4. Evaluate the model using **precision**, **recall**, and **F1-score**.\n",
        "\n",
        "You can later replace the data-loading cell with your real dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "# Imports and configuration\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from datetime import datetime\n",
        "\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import precision_recall_fscore_support, classification_report\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Device selection\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load or create data\n",
        "\n",
        "In this cell we either:\n",
        "- Load your real dataset into a `pandas` DataFrame `df`, **or**\n",
        "- Create a small synthetic dataset for demo purposes.\n",
        "\n",
        "Replace the synthetic-data block with your actual data loading when you have it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Popularity    Energy  Danceability  Positiveness  Speechiness  Liveness  \\\n",
            "0    0.250924  0.046096      0.676816      0.043469     0.116424  0.603866   \n",
            "1    0.917448  0.418780      0.332260      0.283034     0.186282  0.317110   \n",
            "2    0.704983  0.314677      0.745282      0.398213     0.608226  0.728456   \n",
            "3    0.232223  0.441665      0.373021      0.583606     0.100031  0.741352   \n",
            "4    0.322892  0.642927      0.999472      0.281002     0.582225  0.872601   \n",
            "\n",
            "   Acousticness  Instrumentalness    Emotion  \n",
            "0      0.190931          0.668516  Disgusted  \n",
            "1      0.481169          0.069520     Scared  \n",
            "2      0.421758          0.393908  Surprised  \n",
            "3      0.083198          0.126224    Nervous  \n",
            "4      0.789339          0.218088      Bored  \n",
            "\n",
            "Class distribution:\n",
            " Emotion\n",
            "Tense        7844\n",
            "Happy        7836\n",
            "Relaxed      7806\n",
            "Angry        7781\n",
            "Bored        7764\n",
            "Sad          7736\n",
            "Anxious      7637\n",
            "Neutral      7629\n",
            "Scared       7621\n",
            "Disgusted    7617\n",
            "Depressed    7590\n",
            "Surprised    7576\n",
            "Nervous      7563\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# TODO: Replace this with your real data loading\n",
        "\n",
        "FEATURE_COLS = [\n",
        "    \"Popularity\", \"Energy\", \"Danceability\", \"Positiveness\",\n",
        "    \"Speechiness\", \"Liveness\", \"Acousticness\", \"Instrumentalness\",\n",
        "]\n",
        "TARGET_COL = \"Emotion\"\n",
        "\n",
        "# Synthetic example data (for demo). Remove when using real data.\n",
        "num_samples = 100000\n",
        "random_seed = 41\n",
        "np.random.seed(random_seed)\n",
        "\n",
        "synthetic_X = np.random.rand(num_samples, len(FEATURE_COLS))\n",
        "synthetic_emotions = np.random.choice([\"Happy\", \"Sad\", \"Angry\", \"Relaxed\", \"Disgusted\", \"Scared\", \"Surprised\", \"Neutral\", \"Bored\", \"Tense\", \"Nervous\", \"Anxious\", \"Depressed\"], size=num_samples)\n",
        "\n",
        "df = pd.DataFrame(synthetic_X, columns=FEATURE_COLS)\n",
        "df[TARGET_COL] = synthetic_emotions\n",
        "\n",
        "print(df.head())\n",
        "print(\"\\nClass distribution:\\n\", df[TARGET_COL].value_counts())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preprocessing and train/test split\n",
        "\n",
        "Here we:\n",
        "\n",
        "- Encode the `Emotion` labels to integers.\n",
        "- Split the data into **train** and **test** sets (hold-out test set).\n",
        "- Scale the features using `StandardScaler` fitted on the training data only.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train shape: (80000, 8), Test shape: (20000, 8)\n",
            "Number of classes: 13\n"
          ]
        }
      ],
      "source": [
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "df[\"Emotion_encoded\"] = label_encoder.fit_transform(df[TARGET_COL])\n",
        "\n",
        "X = df[FEATURE_COLS].values.astype(np.float32)\n",
        "y = df[\"Emotion_encoded\"].values.astype(np.int64)\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=random_seed, stratify=y\n",
        ")\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "n_features = X_train.shape[1]\n",
        "n_classes = len(np.unique(y))\n",
        "\n",
        "print(f\"Train shape: {X_train.shape}, Test shape: {X_test.shape}\")\n",
        "print(f\"Number of classes: {n_classes}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset and model definitions\n",
        "\n",
        "This section defines:\n",
        "\n",
        "- A `Dataset` wrapper (`EmotionDataset`) for PyTorch.\n",
        "- A small feedforward neural network (`SimpleEmotionNet`) for emotion classification.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EmotionDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.from_numpy(X).float()\n",
        "        self.y = torch.from_numpy(y).long()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "\n",
        "class SimpleEmotionNet(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes):\n",
        "        super(SimpleEmotionNet, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 16),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(16, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training and evaluation helpers\n",
        "\n",
        "We define helper functions to:\n",
        "\n",
        "- Train the model for one epoch.\n",
        "- Evaluate the model and compute **precision**, **recall**, and **F1-score** (macro-averaged).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_one_epoch(model, dataloader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for X_batch, y_batch in dataloader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_batch)\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * X_batch.size(0)\n",
        "\n",
        "    return running_loss / len(dataloader.dataset)\n",
        "\n",
        "\n",
        "def evaluate_model(model, dataloader, device):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in dataloader:\n",
        "            X_batch = X_batch.to(device)\n",
        "            outputs = model(X_batch)\n",
        "            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
        "            all_preds.extend(preds)\n",
        "            all_targets.extend(y_batch.numpy())\n",
        "\n",
        "    all_preds = np.array(all_preds)\n",
        "    all_targets = np.array(all_targets)\n",
        "\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        all_targets, all_preds, average=\"macro\", zero_division=0\n",
        "    )\n",
        "\n",
        "    return precision, recall, f1, all_targets, all_preds\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## K-fold cross-validation on training set\n",
        "\n",
        "We perform k-fold cross-validation on the **training** data only, to estimate generalization performance before evaluating once on the held-out test set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==== Fold 1/5 ====\n",
            "Epoch 1/20 - Loss: 2.5670\n",
            "Epoch 5/20 - Loss: 2.5639\n",
            "Epoch 10/20 - Loss: 2.5618\n",
            "Epoch 15/20 - Loss: 2.5604\n",
            "Epoch 20/20 - Loss: 2.5591\n",
            "Fold 1 - Precision: 0.0790 | Recall: 0.0767 | F1: 0.0585\n",
            "\n",
            "==== Fold 2/5 ====\n",
            "Epoch 1/20 - Loss: 2.5672\n",
            "Epoch 5/20 - Loss: 2.5637\n",
            "Epoch 10/20 - Loss: 2.5618\n",
            "Epoch 15/20 - Loss: 2.5605\n",
            "Epoch 20/20 - Loss: 2.5588\n",
            "Fold 2 - Precision: 0.0742 | Recall: 0.0762 | F1: 0.0672\n",
            "\n",
            "==== Fold 3/5 ====\n",
            "Epoch 1/20 - Loss: 2.5667\n",
            "Epoch 5/20 - Loss: 2.5634\n",
            "Epoch 10/20 - Loss: 2.5615\n",
            "Epoch 15/20 - Loss: 2.5603\n",
            "Epoch 20/20 - Loss: 2.5591\n",
            "Fold 3 - Precision: 0.0746 | Recall: 0.0768 | F1: 0.0680\n",
            "\n",
            "==== Fold 4/5 ====\n",
            "Epoch 1/20 - Loss: 2.5671\n",
            "Epoch 5/20 - Loss: 2.5636\n",
            "Epoch 10/20 - Loss: 2.5618\n",
            "Epoch 15/20 - Loss: 2.5602\n",
            "Epoch 20/20 - Loss: 2.5592\n",
            "Fold 4 - Precision: 0.0704 | Recall: 0.0731 | F1: 0.0657\n",
            "\n",
            "==== Fold 5/5 ====\n",
            "Epoch 1/20 - Loss: 2.5670\n",
            "Epoch 5/20 - Loss: 2.5641\n",
            "Epoch 10/20 - Loss: 2.5625\n",
            "Epoch 15/20 - Loss: 2.5614\n",
            "Epoch 20/20 - Loss: 2.5602\n",
            "Fold 5 - Precision: 0.0814 | Recall: 0.0773 | F1: 0.0581\n",
            "\n",
            "==== Cross-Validation Summary (Macro-Averaged) ====\n",
            "Fold 1: Precision=0.0790, Recall=0.0767, F1=0.0585\n",
            "Fold 2: Precision=0.0742, Recall=0.0762, F1=0.0672\n",
            "Fold 3: Precision=0.0746, Recall=0.0768, F1=0.0680\n",
            "Fold 4: Precision=0.0704, Recall=0.0731, F1=0.0657\n",
            "Fold 5: Precision=0.0814, Recall=0.0773, F1=0.0581\n",
            "\n",
            "Mean over 5 folds - Precision=0.0759, Recall=0.0760, F1=0.0635\n"
          ]
        }
      ],
      "source": [
        "k_folds = 5\n",
        "batch_size = 32\n",
        "num_epochs = 20\n",
        "learning_rate = 1e-3\n",
        "\n",
        "epochs_between_reports = 5\n",
        "\n",
        "skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
        "\n",
        "fold_results = []\n",
        "\n",
        "# Tensorboard logging\n",
        "log_dir = f\"runs/emotion_cv_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "writer = SummaryWriter(log_dir)\n",
        "hparams = {\n",
        "    \"learning_rate\": learning_rate,\n",
        "    \"batch_size\": batch_size,\n",
        "    \"k_folds\": k_folds,\n",
        "    \"num_epochs\": num_epochs,\n",
        "    \"epochs_between_reports\": epochs_between_reports,\n",
        "}\n",
        "writer.add_text(\n",
        "    \"hparams\",\n",
        "    \"\\n\".join(f\"{k}: {v}\" for k, v in hparams.items()),\n",
        ")\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(skf.split(X_train, y_train), 1):\n",
        "    print(f\"\\n==== Fold {fold}/{k_folds} ====\")\n",
        "\n",
        "    X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
        "    y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
        "\n",
        "    train_dataset = EmotionDataset(X_tr, y_tr)\n",
        "    val_dataset = EmotionDataset(X_val, y_val)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    model = SimpleEmotionNet(n_features, n_classes).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Train\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        loss = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
        "        writer.add_scalar(f\"Loss/kcv_train/fold_{fold}\", loss, epoch)\n",
        "        if epoch % epochs_between_reports == 0 or epoch == 1 or epoch == num_epochs:\n",
        "            print(f\"Epoch {epoch}/{num_epochs} - Loss: {loss:.4f}\")\n",
        "            precision, recall, f1, y_true_val, y_pred_val = evaluate_model(model, val_loader, device)\n",
        "            writer.add_scalar(f\"F1_Score/kcv_validation/fold_{fold}\", f1, epoch)\n",
        "\n",
        "    # Evaluate on validation split\n",
        "    print(f\"Fold {fold} - Precision: {precision:.4f} | Recall: {recall:.4f} | F1: {f1:.4f}\")\n",
        "\n",
        "    fold_results.append({\n",
        "        \"fold\": fold,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"f1\": f1,\n",
        "    })\n",
        "\n",
        "print(\"\\n==== Cross-Validation Summary (Macro-Averaged) ====\")\n",
        "for r in fold_results:\n",
        "    print(\n",
        "        f\"Fold {r['fold']}: \"\n",
        "        f\"Precision={r['precision']:.4f}, \"\n",
        "        f\"Recall={r['recall']:.4f}, \"\n",
        "        f\"F1={r['f1']:.4f}\"\n",
        "    )\n",
        "\n",
        "mean_precision = np.mean([r[\"precision\"] for r in fold_results])\n",
        "mean_recall = np.mean([r[\"recall\"] for r in fold_results])\n",
        "mean_f1 = np.mean([r[\"f1\"] for r in fold_results])\n",
        "print(\n",
        "    f\"\\nMean over {k_folds} folds - \"\n",
        "    f\"Precision={mean_precision:.4f}, Recall={mean_recall:.4f}, F1={mean_f1:.4f}\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Final training on full training set and evaluation on test set\n",
        "\n",
        "Here we train a fresh model on the **entire training set** and then evaluate once on the held-out **test set**, printing macro-averaged metrics and a detailed classification report.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Final Model] Epoch 1/20 - Loss: 2.5681\n",
            "[Final Model] Epoch 5/20 - Loss: 2.5611\n",
            "[Final Model] Epoch 10/20 - Loss: 2.5563\n",
            "[Final Model] Epoch 15/20 - Loss: 2.5518\n",
            "[Final Model] Epoch 20/20 - Loss: 2.5479\n",
            "\n",
            "==== Test Set Metrics (Macro-Averaged) ====\n",
            "Precision: 0.0770 | Recall: 0.0756 | F1: 0.0678\n",
            "\n",
            "==== Detailed Classification Report (Test Set) ====\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       Angry       0.06      0.03      0.04       300\n",
            "     Anxious       0.08      0.07      0.08       321\n",
            "       Bored       0.12      0.05      0.08       313\n",
            "   Depressed       0.06      0.04      0.05       315\n",
            "   Disgusted       0.09      0.20      0.12       307\n",
            "       Happy       0.07      0.04      0.05       305\n",
            "     Nervous       0.09      0.04      0.05       305\n",
            "     Neutral       0.08      0.09      0.08       305\n",
            "     Relaxed       0.08      0.14      0.10       297\n",
            "         Sad       0.06      0.17      0.09       318\n",
            "      Scared       0.06      0.03      0.04       304\n",
            "   Surprised       0.08      0.05      0.06       305\n",
            "       Tense       0.05      0.03      0.04       305\n",
            "\n",
            "    accuracy                           0.08      4000\n",
            "   macro avg       0.08      0.08      0.07      4000\n",
            "weighted avg       0.08      0.08      0.07      4000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "train_dataset_full = EmotionDataset(X_train, y_train)\n",
        "test_dataset = EmotionDataset(X_test, y_test)\n",
        "\n",
        "train_loader_full = DataLoader(train_dataset_full, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "final_model = SimpleEmotionNet(n_features, n_classes).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(final_model.parameters(), lr=learning_rate)\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    loss = train_one_epoch(final_model, train_loader_full, criterion, optimizer, device)\n",
        "    if epoch % 5 == 0 or epoch == 1:\n",
        "        print(f\"[Final Model] Epoch {epoch}/{num_epochs} - Loss: {loss:.4f}\")\n",
        "\n",
        "precision, recall, f1, y_true_test, y_pred_test = evaluate_model(final_model, test_loader, device)\n",
        "print(\"\\n==== Test Set Metrics (Macro-Averaged) ====\")\n",
        "print(f\"Precision: {precision:.4f} | Recall: {recall:.4f} | F1: {f1:.4f}\")\n",
        "\n",
        "print(\"\\n==== Detailed Classification Report (Test Set) ====\")\n",
        "print(classification_report(\n",
        "    y_true_test,\n",
        "    y_pred_test,\n",
        "    target_names=label_encoder.classes_,\n",
        "    zero_division=0,\n",
        "))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "iao",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
